{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating dataset for NER task",
   "id": "290dd6b19489fb08"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T20:09:48.594600Z",
     "start_time": "2025-02-24T20:09:46.706947Z"
    }
   },
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The function get_wikipedia_article(animal) sends a request to the Wikipedia API to retrieve the plain text extract of an article for the specified animal. It parses the response and extracts the relevant text from the article. This allows you to obtain a summary of the Wikipedia page for any given animal by passing its name as a parameter.",
   "id": "98eeb5ef825f7c3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T19:55:00.576464Z",
     "start_time": "2025-02-24T19:55:00.568682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_wikipedia_article(animal):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"titles\": animal\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page.get(\"extract\", \"\")"
   ],
   "id": "8d472e78f476efc8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The function process_article(text, animal) processes the article text by splitting it into paragraphs and cleaning each one by removing square brackets and extra spaces. It then checks if the animal's name appears in each paragraph and collects the relevant ones. The function returns up to 7 paragraphs that contain the animal's name for further use.",
   "id": "f312fb897d3a8136"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T19:55:00.606036Z",
     "start_time": "2025-02-24T19:55:00.593717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_article(text, animal):\n",
    "    paragraphs = text.split('\\n')\n",
    "    selected_paragraphs = []\n",
    "    for p in paragraphs:\n",
    "        p = re.sub(r'\\[.*?\\]', '', p)  # Remove content in square brackets\n",
    "        p = re.sub(r'\\s+', ' ', p)  # Replace multiple spaces with a single space\n",
    "        cleaned_paragraph = p.strip()\n",
    "\n",
    "        if cleaned_paragraph and animal.lower() in cleaned_paragraph.lower():\n",
    "            selected_paragraphs.append(cleaned_paragraph)\n",
    "        if len(selected_paragraphs) == 7:\n",
    "            break\n",
    "    return selected_paragraphs"
   ],
   "id": "659dc724e97d6c49",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "animals_names.txt contains the list of animals names",
   "id": "b5330d6340bbea4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T19:55:00.668631Z",
     "start_time": "2025-02-24T19:55:00.655762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "animal_names_file = \"animals_names.txt\"\n",
    "\n",
    "with open(animal_names_file, 'r', encoding='utf-8') as file:\n",
    "    animals = [line.strip() for line in file if line.strip()]"
   ],
   "id": "a7056d2c20ca997",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I retrieve and process Wikipedia articles for each animal in the list. I extract relevant paragraphs containing the animal's name and add them to a dataset with the article text and the animal's name as a label. If no relevant paragraphs are found, I print a message and continue with the next animal.",
   "id": "ece9bb74a5b37a56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = []\n",
    "for animal in animals:\n",
    "    article_text = get_wikipedia_article(animal)\n",
    "    if not article_text:\n",
    "        print(f\"Could not retrieve article for: {animal}\")\n",
    "        continue\n",
    "    paragraphs = process_article(article_text, animal)\n",
    "    if paragraphs:\n",
    "        dataset.append({\"text\": \"\\n\".join(paragraphs), \"label\": animal})\n",
    "    else:\n",
    "        print(f\"No paragraphs found containing the animal name for: {animal}\")\n"
   ],
   "id": "9a54d40338349a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, I save the selected paragraphs and their corresponding animal names into a CSV file for further analysis.",
   "id": "3e826e1187cca724"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T19:56:08.325596Z",
     "start_time": "2025-02-24T19:56:08.311032Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 32,
   "source": [
    "with open(\"dataset.csv\", \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"text\", \"label\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for data in dataset:\n",
    "        writer.writerow(data)\n"
   ],
   "id": "b48adf2a87e0e649"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing of the dataset",
   "id": "936be5d6bf5f55c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will preprocess the dataset by converting it into a tagged format suitable for training a Named Entity Recognition (NER) model. The tagged format consists of sentences where each word is tagged with the corresponding entity label. In this case, the entity label will be \"ANIMAL\" for words that represent the animal's name and \"O\" for all other words.",
   "id": "e9115c8e1f6c3e20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will use the NLTK library to perform lemmatization on the words in the dataset. Lemmatization reduces words to their base or root form, which can help improve the model's performance by reducing the vocabulary size and capturing similar words.",
   "id": "c03fae9d8d9bfa9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T20:09:53.858976Z",
     "start_time": "2025-02-24T20:09:53.840013Z"
    }
   },
   "cell_type": "code",
   "source": "lemmatizer = WordNetLemmatizer()",
   "id": "22500c183954fda8",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Read the dataset from the CSV file into a list of dictionaries.",
   "id": "36e4cf30ed082d74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T20:15:31.308264Z",
     "start_time": "2025-02-24T20:15:31.279399Z"
    }
   },
   "cell_type": "code",
   "source": "data = pd.read_csv(\"dataset.csv\").to_dict(orient='records')",
   "id": "a36218ebabc9441e",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The transform_to_tagged_format function takes a text string and an entity label as input and returns the text in a tagged format. It splits the text into sentences and then into words. Each word is cleaned by removing special characters and lowercasing it. The word is then lemmatized to its base form. If the lemmatized word matches the entity label, it is tagged as \"ANIMAL\"; otherwise, it is tagged as \"O\".",
   "id": "4016f8be4068a06d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T20:15:32.177622Z",
     "start_time": "2025-02-24T20:15:32.169992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transform_to_tagged_format(text, entity_label):\n",
    "    tagged_format = []\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    for sentence in sentences:\n",
    "        sentence_tags = []\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            cleaned_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            lowercased_word = cleaned_word.lower()\n",
    "            lemma = lemmatizer.lemmatize(lowercased_word)\n",
    "            tag = \"ANIMAL\" if lemma == entity_label.lower() else \"O\"\n",
    "            sentence_tags.append((cleaned_word, tag))\n",
    "        tagged_format.append(sentence_tags)\n",
    "    return tagged_format"
   ],
   "id": "cb3102453feec801",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will transform the dataset into a tagged format.",
   "id": "46a83a6f26c880e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T20:15:37.110192Z",
     "start_time": "2025-02-24T20:15:32.497007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tagged_data = [sentence for record in data for sentence in transform_to_tagged_format(record.get(\"text\", \"\"), record.get(\"label\", \"\"))]\n",
    "    "
   ],
   "id": "351a97b511c7c3b3",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will convert the tagged data into a DataFrame and save it to a CSV file for further analysis.",
   "id": "da7fff31946fc3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T20:17:24.382512Z",
     "start_time": "2025-02-24T20:17:24.341942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence_column = [\" \".join([word for word, tag in sentence]) for sentence in tagged_data]\n",
    "tag_column = [\" \".join([tag for word, tag in sentence]) for sentence in tagged_data]\n",
    "tagged_df = pd.DataFrame({'Sentence': sentence_column, 'Tags': tag_column})\n",
    "tagged_df.head()"
   ],
   "id": "20490541bb4a52e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  Albatrosses of the biological family Diomedeid...   \n",
       "1  They range widely in the Southern Ocean and th...   \n",
       "2  They are absent from the North Atlantic althou...   \n",
       "3  Great albatrosses are among the largest of fly...   \n",
       "4  The albatrosses are usually regarded as fallin...   \n",
       "\n",
       "                                                Tags  \n",
       "0  ANIMAL O O O O O O O O O O O O O O O O O O O O...  \n",
       "1                              O O O O O O O O O O O  \n",
       "2  O O O O O O O O O O O O ANIMAL O O O O O O O O...  \n",
       "3  O ANIMAL O O O O O O O O O O O O O O O O O O O...  \n",
       "4           O ANIMAL O O O O O O O O O O O O O O O O  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albatrosses of the biological family Diomedeid...</td>\n",
       "      <td>ANIMAL O O O O O O O O O O O O O O O O O O O O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They range widely in the Southern Ocean and th...</td>\n",
       "      <td>O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They are absent from the North Atlantic althou...</td>\n",
       "      <td>O O O O O O O O O O O O ANIMAL O O O O O O O O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great albatrosses are among the largest of fly...</td>\n",
       "      <td>O ANIMAL O O O O O O O O O O O O O O O O O O O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The albatrosses are usually regarded as fallin...</td>\n",
       "      <td>O ANIMAL O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the tagged dataset to a CSV file.",
   "id": "b05f283ebcaf37a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T20:17:11.159883Z",
     "start_time": "2025-02-24T20:17:11.121740Z"
    }
   },
   "cell_type": "code",
   "source": "tagged_df.to_csv(\"tagged_dataset.csv\", index=False)",
   "id": "ab226879aed3d168",
   "outputs": [],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
